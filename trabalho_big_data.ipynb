{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is3uZ3vDWcVy"
      },
      "source": [
        "# Comandos para realização do trabalho da matéria de Big Data com uso da biblioteca PySpark.\n",
        "\n",
        "Este notebook foi **projetado** para guiar os alunos na realização das práticas de Big Data utilizando PySpark. Certifique-se de seguir cada etapa cuidadosamente para garantir a correta execução das atividades.\n",
        "\n",
        "Seu trabalho começará na célula 5. Execute as 4 primeiras células para iniciar a atividade.\n",
        "\n",
        "## <font color=red>Observação importante:</font>\n",
        "\n",
        "<font color=yellow>Trabalho realizado com uso da biblioteca pandas não será aceito!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxdG7rZPWcV6"
      },
      "source": [
        "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab\n",
        "\n",
        "Aqui, você fará o download do dataset necessário para as atividades. Certifique-se de que o arquivo foi descompactado corretamente antes de prosseguir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w47FXx_rWcV8",
        "outputId": "3a3ab77f-681e-411b-ae69-59453c0612f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-27 19:06:36--  https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49549692 (47M) [application/zip]\n",
            "Saving to: ‘imdb-reviews-pt-br.zip’\n",
            "\n",
            "imdb-reviews-pt-br. 100%[===================>]  47.25M   218MB/s    in 0.2s    \n",
            "\n",
            "2024-11-27 19:06:36 (218 MB/s) - ‘imdb-reviews-pt-br.zip’ saved [49549692/49549692]\n",
            "\n",
            "Archive:  imdb-reviews-pt-br.zip\n",
            "  inflating: imdb-reviews-pt-br.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiY6BY5SWcV_"
      },
      "source": [
        "## Instalação manual das dependências para uso do pyspark no Google Colab\n",
        "\n",
        "Esta etapa garante que todas as bibliotecas necessárias para o PySpark sejam instaladas no Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2ZE0G1hWcWA",
        "outputId": "a0746523-4a70-4bb3-c07f-51982d58f033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMZXg0JOWcWB"
      },
      "source": [
        "## Importar, instanciar e criar a SparkSession\n",
        "\n",
        "A SparkSession é o ponto de entrada para usar o PySpark. Certifique-se de configurar corretamente o nome do aplicativo e o master."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pwjh3eTvWcWC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "appName = \"PySpark Trabalho de Big Data\"\n",
        "master = \"local\"\n",
        "\n",
        "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_HtmSsVWcWE"
      },
      "source": [
        "## Criar spark dataframe do CSV utilizando o método read.csv do spark\n",
        "\n",
        "Não altere este código e use o dataframe imdb_df criado aqui em todo o seu trabalho. A criação de um dataframe diferente deste poderá causar erros na coluna sentiment e isso refletirá em erros de resposta das questões."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XQhJr_cMWcWF"
      },
      "outputs": [],
      "source": [
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZRAGQIyWcWH"
      },
      "source": [
        "# Questão 1\n",
        "\n",
        "Nesta questão, você irá calcular a soma dos IDs para entradas onde o sentimento ('sentiment') é 'neg'.\n",
        "\n",
        "### Objetivo:\n",
        "- Usar a coluna 'sentiment' como chave e somar os valores da coluna 'id'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuTZ9lqsWcWH"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro\n",
        "\n",
        "A função map irá transformar cada linha do dataframe em uma **tupla** (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: coluna 'id' convertida para inteiro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4m33d2kGWcWI"
      },
      "outputs": [],
      "source": [
        "def map1(x):\n",
        "      return (x['sentiment'], int(x['id']))\n",
        "  # Esta função deve retornar uma tupla (sentiment, id).\n",
        "  # o argumento de entrada x é uma linha do dataframe imdb_df\n",
        "  # com os dados em um formato semelhante à um dicionário\n",
        "  # Apague a linha abaixo para iniciar seu código.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAymzEBwWcWJ"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar os IDs por \"sentiment\".\n",
        "\n",
        "A função reduce irá somar os valores dos IDs agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2GwmhoYlWcWJ"
      },
      "outputs": [],
      "source": [
        "def reduceByKey1(x,y):\n",
        "      return x + y\n",
        "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
        "  # x e y são valores que serão somados, pois o reduceByKey receberá\n",
        "  # apenas o segundo elemento da tupla vinda da saída da função map.\n",
        "  # Apague a linha abaixo para iniciar seu código.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CaH6HvRWcWK"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "Aqui, você aplicará as funções de map e reduce ao dataframe Spark para calcular os resultados. Não se esqueça de usar o método `.collect()` para visualizar os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX8SbMP8WcWK",
        "outputId": "d88c2605-7015-4b31-b7e3-ac829acde6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A soma é a seguinte:  [('neg', 459568555)]  / RU: 4025304\n"
          ]
        }
      ],
      "source": [
        "# Linha de código para aplicar o map/reduce no seu dataframe spark\n",
        "resultado = imdb_df.rdd.map(map1).reduceByKey(reduceByKey1).filter(lambda x: x[0] == 'neg').collect()\n",
        "# Coloque aqui o código para imprimir o resultado. Não esqueça seu RU:\n",
        "print(\"A soma é a seguinte: \", resultado, \" / RU: 4025304\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL2QolWLWcWK"
      },
      "source": [
        "# Questão 2:\n",
        "\n",
        "Nesta questão, você irá calcular a diferença no número total de palavras entre textos negativos em português e inglês.\n",
        "\n",
        "### Objetivo:\n",
        "- Contar as palavras em cada idioma (colunas 'text_pt' e 'text_en') para entradas onde o sentimento ('sentiment') é 'neg'.\n",
        "- Subtrair o total de palavras em inglês do total em português."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNbw_BhhWcWL"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave de uma tupla principal e como valor uma outra tupla com a soma das palavras de cada idioma como valor.\n",
        "\n",
        "A função map irá transformar cada linha do dataframe em uma tupla (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: Nova tupla com:\n",
        "  - Elemento 0: soma das palavras da coluna 'text_en'\n",
        "  - Elemento 1: soma das palavras da coluna 'text_pt'\n",
        "\n",
        "OU\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: (soma das palavras da coluna 'text_pt') - (soma das palavras da coluna 'text_en')\n",
        "  \n",
        "\n",
        "Para contar as palavras deve-se primeiro separar os textos em uma lista de palavras para então descobrir o tamanho desta lista.\n",
        "Dicas:\n",
        "\n",
        "1. Use o método .split() e não .split(\" \") de string para separar as palavras em uma lista ou use a função split(coluna de texto, regex) do pyspark com o regex igual à \"[ ]+\" ou \"\\s+\"\n",
        "2. Use len() para descobrir o tamanho da lista de palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "2lXUcdKzWcWL"
      },
      "outputs": [],
      "source": [
        "def map2(x):\n",
        "    if x['sentiment'] == 'neg':  # Considera apenas os textos com sentimento 'neg'\n",
        "        quantidade_de_palavras_em_en = len(x['text_en'].strip().split()) if x['text_en'] else 0\n",
        "        quantidade_de_palavras_em_pt = len(x['text_pt'].strip().split()) if x['text_pt'] else 0\n",
        "        return ('neg', (quantidade_de_palavras_em_en, quantidade_de_palavras_em_pt))\n",
        "  # Coloque aqui o seu código para retornar a tupla necessária.\n",
        "  # Sugerimos o retorno em uma tupla com este formato:\n",
        "  #   (sentiment, (quantidade_de_palavras_em_en, quantidade_de_palavras_em_pt))\n",
        "  #   OU\n",
        "  #   (sentiment, quantidade_de_caracteres_em_pt-quantidade_de_caracteres_em_en)\n",
        "  # Apague a linha abaixo para iniciar seu código.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADe6mfk7WcWM"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar o numero de palavras de cada texto português e inglês por \"sentiment\" (dependerá de como você optou por fazer sua função map2).\n",
        "\n",
        "A função reduce irá somar os valores das quantidades de palavras agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9Uwc4aCkWcWM"
      },
      "outputs": [],
      "source": [
        "def reduceByKey2(x,y):\n",
        "      return (x[0] + y[0], x[1] + y[1])\n",
        "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
        "  # x e y são valores que podem ser ou a tupla vinda da saída da função map\n",
        "  # contendo quantidade de palavras em inglês e português, ou a diferença, a\n",
        "  # depender da sua implementação da função map2.\n",
        "  # Apague a linha abaixo para iniciar seu código.\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8ibA_K5WcWM"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtração.\n",
        "3. Realizar a subtração das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfZsP7GZWcWN",
        "outputId": "fa538770-9139-4a90-eb4a-9ad25079c1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diferença no número total de palavras (português - inglês) para textos negativos: 54949 RU: 4025304\n"
          ]
        }
      ],
      "source": [
        "# Linha de código para aplicar o map/reduce no seu dataframe spark\n",
        "resultado = imdb_df.rdd.map(map2).filter(lambda x: x is not None).reduceByKey(reduceByKey2).filter(lambda x: x[0] == 'neg').collect()\n",
        "\n",
        "(_,(total_palavras_en, total_palavras_pt)) = resultado[0]\n",
        "diferenca = total_palavras_pt - total_palavras_en\n",
        "# Coloque aqui suas linhas de código final para imprimir o resultado.\n",
        "# Não esqueça seu RU:\n",
        "print(f\"Diferença no número total de palavras (português - inglês) para textos negativos: {diferenca}\", \"RU: 4025304\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}